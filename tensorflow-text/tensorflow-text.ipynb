{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'\\x00E\\x00v\\x00e\\x00r\\x00y\\x00t\\x00h\\x00i\\x00n\\x00g\\x00 \\x00n\\x00o\\x00t\\x00 \\x00s\\x00a\\x00v\\x00e\\x00d\\x00 \\x00w\\x00i\\x00l\\x00l\\x00 \\x00b\\x00e\\x00 \\x00l\\x00o\\x00s\\x00t\\x00.',\n",
       "       b'\\x00S\\x00a\\x00d&9'], dtype=object)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'Everything not saved will be lost.', b'Sad\\xe2\\x98\\xb9'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list())\n",
    "print(offset_limits.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, False, False, False, False, False, False], [True, False]]\n",
      "[[False, False, False, False, False, False, False], [False, False]]\n",
      "[[False, False, False, False, False, False, False], [False, False]]\n",
      "[[False, False, False, False, False, False, False], [False, False]]\n"
     ]
    }
   ],
   "source": [
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
